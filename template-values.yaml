# template-values.yaml: Helm values template for coder-saas code-server chart
# This file uses placeholder variables replaced by deploy-code-server.sh
# Placeholders: ${STORAGE_SIZE}, ${CODE_SERVER_PASSWORD}, ${SERVICE_TYPE}, ${GPU_ENABLED}, ${GPU_COUNT}

# Namespace for the deployment
namespace: "code-server"

# Global settings for defaults
global:
  # PersistentVolumeClaim configuration
  persistentVolumeClaim:
    size: "${STORAGE_SIZE}"  # Override with desired size

  # Secret for code-server password
  secret:
    name: "code-server-password"
    password: "${CODE_SERVER_PASSWORD}"  # Override with your password

# Deployment configuration for code-server
deployment:
  image:
    repository: "ghcr.io/coder/code-server"
    tag: "latest"
  replicas: 1
  env:
    - name: PASSWORD
      valueFrom:
        secretKeyRef:
          name: code-server-password  # Reference to the secret above
          key: password
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "1000m"
      memory: "1Gi"
  # Volume mount for persistent storage
  volumeMounts:
    - name: data-vol
      mountPath: "/home/coder/project"
  volumes:
    - name: data-vol
      persistentVolumeClaim:
        claimName: "code-server-pvc"  # Reference to PVC below

# PersistentVolumeClaim
persistentVolumeClaim:
  name: "code-server-pvc"
  size: "${STORAGE_SIZE}"  # Uses global.persistentVolumeClaim.size
  accessModes:
    - "ReadWriteOnce"
  storageClassName: ""  # Leave empty for default

# Secret for code-server password
secret:
  name: "code-server-password"
  type: "Opaque"
  data:
    password: "${CODE_SERVER_PASSWORD}"  # Base64 encoded if needed

# Service configuration
service:
  name: "code-server-service"
  type: "${SERVICE_TYPE}"  # LoadBalancer or ClusterIP
  ports:
    - name: http
      port: 80
      targetPort: 8080
  annotations:
    kubernetes.io/ingress.class: "nginx"  # Example annotation for ingress integration

# GPU Support Configuration (enabled when GPU server classes are selected)
gpu:
  enabled: ${GPU_ENABLED:-false}  # Set to true when deploying to GPU-enabled node pools
  nodeSelector:
    nvidia.com/gpu: "present"  # Select nodes with NVIDIA GPUs
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Equal"
      value: "present"
      effect: "NoSchedule"
  # NVIDIA GPU Device Plugin DaemonSet (required for GPU workloads)
  devicePlugin:
    enabled: ${GPU_DEVICE_PLUGIN_ENABLED:-false}  # Enable NVIDIA device plugin when using GPU nodes
    image:
      repository: "nvcr.io/nvidia/k8s-device-plugin"
      tag: "v0.14.1"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: "50m"
        memory: "100Mi"
      limits:
        cpu: "100m"
        memory: "200Mi"
    args:
      - "--pass-device-specs"
      - "--fail-on-init-error"
      - "--device-list-strategy=envvar"
  # GPU Resource Requests/Limits for code-server pod
  resources:
    limits:
      nvidia.com/gpu: ${GPU_COUNT:-0}  # Request GPUs based on server class selection
    requests:
      nvidia.com/gpu: ${GPU_COUNT:-0}

# Spot-specific configurations
spot:
  # Webhook endpoints for preemption monitoring
  webhooks:
    preemption:
      url: "https://webhook.example.com/preemption-handler"
      events: ["preemption"]
      warningMinutes: 5
      enabled: false

  # Bid management settings
  bid:
    strategy: "optimized"
    markup: 0.10
    minBid: 0.01
    maxBid: 1.0

  # Graceful shutdown settings
  shutdown:
    preemptionGracePeriod: 300
    webhookTimeout: 10

  # Market monitoring
  monitoring:
    priceUpdateInterval: 300
    bidAdjustmentThreshold: 0.05

# Additional configurations for idempotency
metadata:
  labels:
    app: code-server
    version: v1.0
    spot-enabled: "true"